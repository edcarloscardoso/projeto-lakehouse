{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e88039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazySparkSession:\n",
    "    packages = [\n",
    "        \"io.delta:delta-spark_2.12:3.3.1\",\n",
    "        \"io.unitycatalog:unitycatalog-spark_2.12:0.2.0\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "        # \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "    ]\n",
    "\n",
    "    def start(\n",
    "        self,\n",
    "        app_name: str = \"LazySparkSession App\",\n",
    "        executor_memory: str = \"1g\",\n",
    "        driver_memory: str = \"1g\",\n",
    "        driver_maxresultsize: str = \"1g\",\n",
    "        master_url: str = \"local[1]\",\n",
    "    ):\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        builder = (\n",
    "            SparkSession.Builder().appName(app_name)\n",
    "            .config(\"spark.executor.memory\", executor_memory)\n",
    "            .config(\"spark.driver.memory\", driver_memory)\n",
    "            .config(\"spark.driver.maxResultSize\", driver_maxresultsize)\n",
    "            .config(\"spark.jars.packages\", \",\".join(self.packages))\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"io.unitycatalog.spark.UCSingleCatalog\")\n",
    "            .config(\"spark.sql.catalog.unity\", \"io.unitycatalog.spark.UCSingleCatalog\")\n",
    "            .config(\"spark.sql.catalog.unity.uri\", \"http://127.0.0.1:8080\")\n",
    "            .config(\"spark.sql.catalog.unity.token\", \"\")\n",
    "            .config(\"spark.sql.defaultCatalog\", \"unity\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            # .config(\"spark.hadoop.fs.s3a.access.key\", \"guQHMBGAXnfsVlgfB6ro\")\n",
    "            # .config(\"spark.hadoop.fs.s3a.secret.key\", \"jlcXrCCxYDjkXttPrlPFLwb08l58v5tJflooKuQU\")\n",
    "            # .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://127.0.0.1:10000\")\n",
    "            .master(master_url)\n",
    "        )\n",
    "\n",
    "        return builder.getOrCreate()\n",
    "    \n",
    "\n",
    "\n",
    "# AWS_ACCESS_KEY=\"*****\"\n",
    "# AWS_SECRET_KEY=\"******\"\n",
    "# conf = (\n",
    "#     SparkConf()\n",
    "#     .set(\"spark.executor.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\")\n",
    "#     .set(\"spark.driver.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\")\n",
    "#     .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\n",
    "#     .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\n",
    "# )\n",
    "\n",
    "# scT=SparkContext(conf=conf)\n",
    "# sql = SparkSession(scT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b69fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/06/04 19:09:32 WARN Utils: Your hostname, PC21-107632 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/06/04 19:09:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/elicacio/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/elicacio/.ivy2/cache\n",
      "The jars for the packages stored in: /home/elicacio/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "io.unitycatalog#unitycatalog-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3b276223-d0c5-4957-984f-901c6ec25625;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.1 in central\n",
      "\tfound io.delta#delta-storage;3.3.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound io.unitycatalog#unitycatalog-spark_2.12;0.2.0 in central\n",
      "\tfound io.unitycatalog#unitycatalog-client;0.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.13 in central\n",
      "\tfound org.apache.logging.log4j#log4j-slf4j2-impl;2.23.1 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.23.1 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.23.1 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 in central\n",
      "\tfound org.openapitools#jackson-databind-nullable;0.2.6 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.0 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;6.5.1 in central\n",
      "\tfound org.antlr#antlr4;4.9.3 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.3.1 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;69.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.4 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.1/delta-spark_2.12-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.3.1!delta-spark_2.12.jar (877ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (8292ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.1/delta-storage-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.3.1!delta-storage.jar (189ms)\n",
      ":: resolution report :: resolve 3656ms :: artifacts dl 9411ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;6.5.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;69.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.3.1 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.1 from central in [default]\n",
      "\tio.unitycatalog#unitycatalog-client;0.2.0 from central in [default]\n",
      "\tio.unitycatalog#unitycatalog-spark_2.12;0.2.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.3.1 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.9.3 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.0 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.23.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.23.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-slf4j2-impl;2.23.1 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.openapitools#jackson-databind-nullable;0.2.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;2.0.9 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.17.0 by [com.fasterxml.jackson.core#jackson-annotations;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.17.0 by [com.fasterxml.jackson.core#jackson-core;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.17.0 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.0-rc2 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   39  |   3   |   3   |   6   ||   33  |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3b276223-d0c5-4957-984f-901c6ec25625\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 30 already retrieved (281052kB/480ms)\n",
      "25/06/04 19:09:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 19:10:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = LazySparkSession().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc5f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|  default|        marksheet|      false|\n",
      "|  default|marksheet_uniform|      false|\n",
      "|  default|          numbers|      false|\n",
      "|  default|   user_countries|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schemas (output = default)\n",
    "spark.sql(\"SHOW SCHEMAS\").show()\n",
    "\n",
    "# Show tables\n",
    "spark.sql(\"SHOW TABLES IN default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7f2e18",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_TABLE_NOT_FOUND] Delta table `default`.`marksheet` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM default.marksheet LIMIT 5;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DELTA_TABLE_NOT_FOUND] Delta table `default`.`marksheet` doesn't exist."
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM default.marksheet LIMIT 5;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f11cdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     demo|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new schema\n",
    "spark.sql(\"CREATE SCHEMA demo\")\n",
    "\n",
    "# Should now show two schemas: default and demo\n",
    "spark.sql(\"SHOW SCHEMAS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4832376e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'STORAGE_LOCATION'.(line 4, pos 0)\n\n== SQL ==\n\nCREATE TABLE demo.mytable (id INT, desc STRING)\nUSING delta\nSTORAGE_LOCATION 's3a://landing/tables/mytable'\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a new table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[33;43mCREATE TABLE demo.mytable (id INT, desc STRING)\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43mUSING delta\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43mSTORAGE_LOCATION \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3a://landing/tables/mytable\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Example location:\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# LOCATION '/tmp/tables/mytable'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mParseException\u001b[39m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'STORAGE_LOCATION'.(line 4, pos 0)\n\n== SQL ==\n\nCREATE TABLE demo.mytable (id INT, desc STRING)\nUSING delta\nSTORAGE_LOCATION 's3a://landing/tables/mytable'\n^^^\n"
     ]
    }
   ],
   "source": [
    "# Create a new table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE demo.mytable (id INT, desc STRING)\n",
    "USING delta\n",
    "STORAGE_LOCATION 's3a://landing/tables/mytable'\n",
    "\"\"\")\n",
    "# Example location:\n",
    "# LOCATION '/tmp/tables/mytable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbf8d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_PATH_DOES_NOT_EXIST] file:/home/unitycatalog/s3a:/landing/tables/mytable doesn't exist, or is not a Delta table.;\nDescribeDeltaDetailCommand\n+- ResolvedTable io.unitycatalog.spark.UCSingleCatalog@7af9ecec, demo.mytable, DeltaTableV2(org.apache.spark.sql.SparkSession@202c19c2,file:/home/unitycatalog/s3a:/landing/tables/mytable,Some(CatalogTable(\nCatalog: unity\nDatabase: demo\nTable: mytable\nCreated Time: Wed Jun 04 19:14:07 BRT 2025\nLast Access: UNKNOWN\nCreated By: Spark \nType: EXTERNAL\nProvider: DELTA\nLocation: file:///home/unitycatalog/s3a:/landing/tables/mytable)),Some(demo.mytable),None,Map())\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDESCRIBE DETAIL demo.mytable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DELTA_PATH_DOES_NOT_EXIST] file:/home/unitycatalog/s3a:/landing/tables/mytable doesn't exist, or is not a Delta table.;\nDescribeDeltaDetailCommand\n+- ResolvedTable io.unitycatalog.spark.UCSingleCatalog@7af9ecec, demo.mytable, DeltaTableV2(org.apache.spark.sql.SparkSession@202c19c2,file:/home/unitycatalog/s3a:/landing/tables/mytable,Some(CatalogTable(\nCatalog: unity\nDatabase: demo\nTable: mytable\nCreated Time: Wed Jun 04 19:14:07 BRT 2025\nLast Access: UNKNOWN\nCreated By: Spark \nType: EXTERNAL\nProvider: DELTA\nLocation: file:///home/unitycatalog/s3a:/landing/tables/mytable)),Some(demo.mytable),None,Map())\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE DETAIL demo.mytable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d55df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_TABLE_NOT_FOUND] Delta table `demo`.`mytable` doesn't exist.;\nAppendData RelationV2[] unity.demo.mytable unity.demo.mytable, false\n+- LocalRelation [col1#184, col2#185]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Insert new rows\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mINSERT INTO demo.mytable VALUES (1, \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest 1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# spark.sql(\"INSERT INTO demo.mytable VALUES (2, 'test 2')\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# spark.sql(\"INSERT INTO demo.mytable VALUES (3, 'test 3')\")\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# spark.sql(\"INSERT INTO demo.mytable VALUES (4, 'test 4')\")\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# # Read table\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# spark.sql(\"SELECT * FROM demo.mytable\").show()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DELTA_TABLE_NOT_FOUND] Delta table `demo`.`mytable` doesn't exist.;\nAppendData RelationV2[] unity.demo.mytable unity.demo.mytable, false\n+- LocalRelation [col1#184, col2#185]\n"
     ]
    }
   ],
   "source": [
    "# Insert new rows\n",
    "spark.sql(\"INSERT INTO demo.mytable VALUES (1, 'test 1')\")\n",
    "spark.sql(\"INSERT INTO demo.mytable VALUES (2, 'test 2')\")\n",
    "spark.sql(\"INSERT INTO demo.mytable VALUES (3, 'test 3')\")\n",
    "spark.sql(\"INSERT INTO demo.mytable VALUES (4, 'test 4')\")\n",
    "\n",
    "# Read table\n",
    "spark.sql(\"SELECT * FROM demo.mytable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99cd21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
